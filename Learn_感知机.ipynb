{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 感知机\n",
    "\n",
    "## 感知机的作用\n",
    "\n",
    "​\t感知机是一个二分类的线性分类模型，输入为实例的特征向量，输出是+1或-1的一个类别，感知机在特征空间中，将实例划分为正负两类的分离超平面，属于判别模型。\n",
    "\n",
    "\n",
    "## 感知机的模型\n",
    "\n",
    "​\t输入空间$\\mathcal{X}\\subset{R^n}$,是一个$n$维向量的集合，输出空间$\\mathcal{Y}=\\{-1,+1\\}$是一个类标记的集合。\n",
    "\n",
    "输入$x\\in\\mathcal{X}$表示实例的特征向量，$y\\in\\mathcal{Y}$表示实例的类别。输入到输出如下函数：\n",
    "$$\n",
    "f(x)=sign(wx+b)\n",
    "$$\n",
    "\n",
    "\n",
    "​\t如果能够将数据集的正实例点和负实例点完全划分到超平面两侧，也就是对所有$y_i=+1$的实例$i$，有$wx_i+b>0$,$y_i=-1$的实例$i$，有$wx_i+b<0$,则这个数据集是个线性可分的，否则就是线性不可分。如图，数据集对应于特征空间中的超平面$S$，使用$wx+b=0$将特征向量分为正负两类\n",
    "\n",
    "<img src=\"./配图/感知机.assets/感知机.png\" alt=\"iShot2022-03-03 15.52.27\" style=\"zoom:50%;\" />\n",
    "\n",
    "## 感知机的学习策略\n",
    "\n",
    "​\t首先确保数据集是线性可分的，感知器通过改变$w,b$来寻找到这样的一个超平面，并将损失函数极小化。\n",
    "\n",
    "​\t损失函数计算的是误分类点到超平面的总距离,因为是误分类点，所以$wx_i+b$>0时，$y_i<0$，所以相乘是一个负数，需要在求和前加上一个负号使得损失函数变正:\n",
    "$$\n",
    "L(w,b)=-\\sum y_i(wx_i+b)\n",
    "$$\n",
    "\n",
    "## 感知器算法（原始形式）\n",
    "\n",
    "​\t1.随机选取初$w_0,b_0$\n",
    "\n",
    "​\t2.在训练集中选取数据\n",
    "\n",
    "​\t3.如果这是一个误分类点，即$ y_i(wx_i+b)\\leq0$,则进行参数更新：\n",
    "$$\n",
    "w = w+\\eta y_ix_i\\\\\n",
    "b = b+\\eta y_i\n",
    "$$\n",
    "​\t4.重复（2），直至没有误分类点\n",
    "\n",
    "\n",
    "\n",
    "## 感知器算法（对偶形式）\n",
    "\n",
    "### 思想：\n",
    "\n",
    "​\t对偶形式的想法就是，将$w$和$b$，标记为实例$x_i$和$y_i$的线性组合，通过求解系数来求得$w$和$b$。当我们逐步修改n次$w$和$b$时，$w$和$b$关于$(x_i,y_i)$的增量可以表示为\n",
    "$$\n",
    "w = \\sum_{i=1}^N\\alpha_iy_ix_i\\\\\n",
    "b=\\sum_{i=1}^N\\alpha_iy_i\\\\\n",
    "这里的\\alpha_i=\\eta_i\\eta\\geq0\n",
    "$$\n",
    "​\t实例点的更新次数越多，意味着他距离超平面就越近，也就越难正确分类，这样的实例对学习结果的影响最大，对偶形式也是对算法执行速度的优化。\n",
    "\n",
    "\n",
    "\n",
    "### 算法：\n",
    "\n",
    "​\t1.随机选取初$\\alpha\\leftarrow0,b\\leftarrow0$\n",
    "\n",
    "​\t2.在训练集中选取数据\n",
    "\n",
    "​\t3.如果这是一个误分类点，即$ y_i(\\sum_{j=1}^N\\alpha_jy_jx_jx_i+b)\\leq0$,则进行参数更新：\n",
    "$$\n",
    "\\alpha_i=\\alpha_i+\\eta\\\\\n",
    "b = b+\\eta y_i\n",
    "$$\n",
    "​\t4.重复（2），直至没有误分类点\n",
    "\n",
    "​\t\t\t\t\t\t因为对偶训练实例仅以内积形式出现，为了方便，可以预先将训练实例的内积算出\t\t\t\t\t并存储为一个矩阵，也就是$Gram$矩阵，每次计算时候调用矩阵节省时间。\n",
    "$$\n",
    "G = [x_i\\cdot x_j]_{N\\times N}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 神经网络解决方案\n",
    "\n",
    "试设计一个前馈神经网络来解决XOR问题，要求该前馈神经网络具有 两个隐藏神经元和一个输出神经元，并使用 ReLU 作为激活函数."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 0])\n",
      "CE_Loss 9.27700602915138e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])# 四个位置\n",
    "label = torch.tensor([0,1,1,0]) # 可以看出这样的label情况下，我们并不能简单的用一条直线来分割。\n",
    "\n",
    "class XOR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XOR, self).__init__()\n",
    "        self.hidden1 = nn.Linear(2, 32)\n",
    "        self.hidden2 = nn.Linear(32, 16)\n",
    "        self.hidden3 = nn.Linear(16, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.relu(self.hidden2(x))\n",
    "        x = self.relu(self.hidden3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "model = XOR()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "total_accuracy = 0\n",
    "for epoch in range(500):\n",
    "    output = model(input)\n",
    "    loss = loss_function(output, label)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "print(output.argmax(1))\n",
    "print('CE_Loss', loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
