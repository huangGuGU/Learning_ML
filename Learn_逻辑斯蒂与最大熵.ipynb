{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 逻辑斯蒂回归模型\n",
    "\n",
    "- 逻辑斯蒂分布\n",
    "\n",
    "  ​\t逻辑斯蒂的概率密度函数$f(x)$和分布函数$F(x)=\\frac{1}{1+e^{-(x-\\mu)/\\gamma}}$。\n",
    "\n",
    "![image-20220309224019619](./配图/逻辑斯蒂和最大熵.assets/image-20220309224019619.png)\n",
    "\n",
    "\n",
    "\n",
    "#### 二项逻辑斯蒂回归模型\n",
    "\n",
    "​\t这是一种分类模型，由条件概率分布的$P(Y|X)$表示，形式为参数化的逻辑斯蒂分布，$X$是一个随机变量的实数，$Y$是0-1，通过监督学习方法来估计模型参数。\n",
    "\n",
    "- 逻辑斯蒂回归模型\n",
    "  $$\n",
    "  P(Y=1|x)=\\frac{e^{w\\cdot x}}{1+e^{w\\cdot x}}\\\\\n",
    "  P(Y=0|x)=\\frac{1}{1+e^{w\\cdot x}}\\\\\n",
    "  w  =(w^{(1)}，w^{(2)}，...,w^{(n)},b)^T\\\\\n",
    "  x = (x^{(1)},x^{(2)},...,x^{(n)},1)^T\n",
    "  $$\n",
    "  通过给定的$x$,按照上式就可以求得$Y=1,Y=0$的概率，将$x$分到概率大的一类。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 逻辑斯蒂回归模型的特点\n",
    "\n",
    "  ​\t一个事件的几率是指发生的概率与不发生的概率的比值，所以该事件的对数几率或者logit函数是：\n",
    "  $$\n",
    "  logit(p)=log(\\frac{p}{1-p})\n",
    "  $$\n",
    "  ​\t对逻辑斯蒂回归而言，则可得：\n",
    "  $$\n",
    "  log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x\n",
    "  $$\n",
    "\n",
    "\n",
    "  ​\t可以看出，输出$Y=1$的对数几率是$x$的线性函数，可以用输入为$x$的线性函数来表示，这样的模型就是逻辑斯蒂模型。\n",
    "\n",
    "  ​\t如果是使用\n",
    "  $$\n",
    "  P(Y=1|x)=\\frac{e^{w\\cdot x}}{1+e^{w\\cdot x}}\n",
    "  $$\n",
    "\n",
    "​\t\t\t则可以看出线性函数的值越接近正无穷，概率值就越接近1，相反，越接近负无穷，概率值就越接近0，这\t\t样的模型就是逻辑斯蒂模型。\n",
    "\n",
    "\n",
    "\n",
    "#### 最大熵模型\n",
    "\n",
    "- 最大熵原理\n",
    "\n",
    "  ​\t最大熵原理认为，学习概率模型时，在所有可能的模型分布中，熵最大的模型就是最好的模型，通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。\n",
    "\n",
    "  ​\t直观的，我们可以认为，我们的模型在满足以后的事实，即约束条件下，在没有更多信息时，我们将那些不确定的部分都认为是等概率的，因为我们知道，熵的最大化就表示等可能（由熵的计算公式得来），而等可能不好做操作，而熵却是一个可以优化的数值。\n",
    "\n",
    "\n",
    "\n",
    "- 最大熵模型的定义\n",
    "\n",
    "  ​\t当我们有一个训练集后，我们可以知道的是，其联合分布$P(X,Y)$的经验分布$\\tilde{P}(X,Y)=\\frac{v(X=x,Y=y)}{N}$，和边缘分布$P(X)$的经验分布$\\tilde{P}(X)=\\frac{v(X=x)}{N}$，$v$统计出现这种情况的频数。\n",
    "\n",
    "  ​\t特征函数来表示$x,y$之间的某一个事实。\n",
    "  $$\n",
    "  \\begin{equation}\n",
    "  f(x,y)=\n",
    "  \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "  1 & x,y满足\\\\\n",
    "  0 & else\n",
    "  \\end{array}\n",
    "  \\right.\n",
    "  \\end{equation}\n",
    "  $$\n",
    "  ​\t于是我们可以用特征函数$f(x,y)$关于经验分布$\\tilde{P}(X,Y)$的期望值$E_{\\tilde{P}}(f)$:\n",
    "  $$\n",
    "  E_{\\tilde{p}}(f)=\\sum_{x,y}\\tilde{P}(x,y)f(x,y)\n",
    "  $$\n",
    "\n",
    "​\t\t\t特征函数关于模型$P(X,Y)$与经验分布$\\tilde{P}(X)$的期望值，$E_{P}(f)$:\n",
    "$$\n",
    "E_{P}(f) =\\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)\n",
    "$$\n",
    "​\t\t\t如果能从训练数据中获取信息，那么就可以假设上述两个期望值相等。\n",
    "$$\n",
    "E_{\\tilde{p}}(f)=E_{P}(f) \\\\\n",
    "\\sum_{x,y}\\tilde{P}(x,y)f(x,y)=\\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y)\n",
    "$$\n",
    "​\t\t\t我们可以将这个作为我们模型的约束条件，加入有n个特征函数，那我们就有n个约束条件。\n",
    "\n",
    "\n",
    "\n",
    "- 最大熵模型\n",
    "\n",
    "  ​\t假设满足所有约束条件的模型集合是：\n",
    "  $$\n",
    "  \\mathcal{C}\\equiv\\{ P\\in\\mathcal{P}|E_{\\tilde{p}}(f_i)=E_{P}(f_i) \\}\n",
    "  $$\n",
    "  ​\t定义在概率分布$P(Y|X)$的条件熵为：\n",
    "  $$\n",
    "  H(P)=-\\sum_{x,y}\\tilde{P}(x)P(y|x)lnP(y|x)\n",
    "  $$\n",
    "  ​\t在集合$\\mathcal{C}$中熵最大的模型就是最大熵模型。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 最大熵模型的学习\n",
    "\n",
    "  ​\t 这个学习过程就是求解最大熵模型的过程，可以理解为约束最优化的问题。\n",
    "\n",
    "  对于给定的数据集和特征函数后，最大熵的学习为：\n",
    "  $$\n",
    "  max_{P\\in C}H(P)=-\\sum_{x,y}\\tilde{P}(x)P(y|x)lnP(y|x)\\\\\n",
    "  限定条件：E_{\\tilde{p}}(f_i)=E_{P}(f_i),\\sum_yP(y|x)=1\n",
    "  $$\n",
    "  将求最大值变成求最小值：\n",
    "  $$\n",
    "  min_{P\\in C}-H(P)=\\sum_{x,y}\\tilde{P}(x)P(y|x)lnP(y|x)\\\\\n",
    "  限定条件：E_{\\tilde{p}}(f_i)=E_{P}(f_i),\\sum_yP(y|x)=1\n",
    "  $$\n",
    "  于是，我们引入拉格朗日乘子$w_0,w_1,...,w_n$:\n",
    "  $$\n",
    "  L(P,w) = -H(P)+w_0(1-\\sum_yP(y|x))+\\sum_{i=1}^n w_i(E_{\\tilde{p}}(f_i)-E_{P}(f_i))\n",
    "  $$\n",
    "  原问题是：$min_{P\\in C}max_w L(P,w)$,对偶问题就是：$max_wmin_{P\\in C} L(P,w)$。\n",
    "\n",
    "\n",
    "\n",
    "  ​\t为什么要$max_w$呢？\n",
    "\n",
    "  ​\t因为如果不满足约束条件的话，$w\\rightarrow+\\infin$时，$L(P,w)\\rightarrow+\\infin$，但我们需要的是最小值，所以约束必须要满足，满足约束后可得$L(P,w)=max_wL(P,w)$\n",
    "\n",
    "​\n",
    "\n",
    "​\t\t求解$min_{P\\in C} L(P,w)$问题就是$w$的函数,记作：\n",
    "$$\n",
    "\\Psi(w)=min_{P\\in C} L(P,w)=L(P_w,w)\n",
    "$$\n",
    "​\t\t这个问题的解也可以记为：\n",
    "$$\n",
    "P_w = argmin_{P\\in C}L(P,w)=P_w(y|x)\n",
    "$$\n",
    "​\t\t具体可以求$L(P,w)$对$P(y|x)$的偏导数:\n",
    "$$\n",
    "\\frac{\\partial L(P,w)}{\\partial P(y|x)}=0\n",
    "$$\n",
    "​\t\t解得：\n",
    "$$\n",
    "P_w(y|x)=\\frac{1}{Z_w(x)}e^{\\sum_{i=1}^nw_if_i(x,y)}\\\\\n",
    " 规范化因子：Z_w(x)=\\sum_y e^{\\sum_{i=1}^nw_if_i(x,y)}\n",
    "$$\n",
    "​\t\t这个$P_w(y|x)$就是最大熵模型，里面的$w$就是这个模型中的参数向量。\n",
    "\n",
    "​\t\t之后就是求解对偶问题的外部极大化问题\n",
    "$$\n",
    "max_w\\Psi(w)\n",
    "$$\n",
    "​\t\t这个解记为$w*$:\n",
    "$$\n",
    "w*=argmax_w\\Psi(w)\n",
    "$$\n",
    "​\t\t得到的$w*$用来表示$P*\\in C$，$P*$就是学到的最优模型（最大熵模型），也就是说可以把问题归结为求对偶函\t数$\\Psi(w)$的极大化。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
