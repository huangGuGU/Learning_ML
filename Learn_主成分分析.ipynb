{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 主成分分析PCA\n",
    "\n",
    "## 背景\n",
    "\n",
    "  许多机器学习问题涉及每个训练实例的成千上万甚至数百万个特征。正如我们将看到的那样，所有这些特征不仅使**训练变得极其缓慢**，而且还会使找到好的解决方案变得更加困难，大多数训练实例可能彼此之间相距很远。当然，这也意味着新的实例很可能远离任何一个训练实例，导致跟 低维度相比，预测更加不可靠，因为它们基于更大的推测。简而言之， **训练集的维度越高，过拟合的风险就越大**。这个问题通常称为维度的诅咒，所以我们需要用一些方法降低维度。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 原理\n",
    "\n",
    "  它识别最靠近数据的超平面，然后将数据投影到上面，如图的样本点为ABC，找到一个超平面y1，使得三个垂线的总和是最小的，最小的轴是对差异性贡献度最高的，其次选择第二条第三条等等。\n",
    "\n",
    "  <img src=\"./配图/降维.assets/image-20220421上午112801965.png\" alt=\"image-20220421上午112801965\" style=\"zoom:50%;\" />\n",
    "\n",
    "  ​\t使用SVD工具来寻找训练集的主要成分，SVD可以将训练集矩阵 $X$ 分解成 $U\\sum V^T$ 的矩阵算法\n",
    "\n",
    "  <img src=\"./配图/降维.assets/image-20220421上午113218136.png\" alt=\"image-20220421上午113218136\" style=\"zoom:50%;\" />\n",
    "\n",
    "\n",
    "\n",
    "​\t$X_{2d} =XW_d $ 这样，我们的维度就变成和 $W_d$ 一样了。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''numpy 实现'''\n",
    "import numpy as np\n",
    "X = np.random.rand(500,500)\n",
    "X_centered = X- X.mean(axis=0) # 数据规范化\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "# 提取前两个单位向量\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]\n",
    "\n",
    "# W2就是前两个向量的组合，通过W2进行映射\n",
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.00780391, 0.00768121])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''sklearn实现'''\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2) # components_属性是Wd的转置\n",
    "X2D = pca.fit_transform(X)\n",
    "\n",
    "'''\n",
    "可解释方差比\n",
    "该比率表示沿每个成分的数 据集方差的比率。\n",
    "通过比率可以看出哪部分的信息多\n",
    "'''\n",
    "pca.explained_variance_ratio_\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "选择正确的维度\n",
    "\n",
    "与其任意选择要减小到的维度，不如选择相加足够大的方差部分 (例如95%)的维度。当然，如果你是为了数据可视化而降低维度，这 种情况下，需要将维度降低到2或3。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "305"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1 # d就是需要保存的最少维度\n",
    "d"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "也可以将n_components设置为0.0到1.0之间的浮点数来表示要保 留的方差率"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "305"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "#这时候查看pca.n_components_\n",
    "pca.n_components_\n",
    "#得到的结果和上面的d一致"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0.9502812356632158"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.explained_variance_ratio_) #输出来确认是否满足我们需要的可解释方差"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PCA压缩\n",
    "\n",
    "降维过后的训练集占用空间也少了，同样少了维度后是的分类算法的运算速度也加快，也就是通过PCA对数据进行压缩，也可以进行解压缩回到之前的特征数，但是解压后的数据与原始数据有误差，这之前的均方距离就成为重构误差。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)  # 压缩到154个特征\n",
    "X_reduced = pca.fit_transform(X)\n",
    "X_recovered = pca.inverse_transform(X_reduced) # 解压缩回去"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 随机PCA\n",
    "\n",
    "如果我们的特征数太多，m或n大于500个，并且需要的特征数小于80%，则会自动使用随机PCA算法，该算法可以快速找到前d个 主成分的近似值。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# 如果要强制使用完全的SVD可以设置为‘full’\n",
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 增量PCA\n",
    "\n",
    "前面的PCA实现的一个问题是，它们要求整个训练集都放入内存才能运行算法。幸运的是已经开发了增量PCA(IPCA)算法，它们可以使 你把训练集划分为多个小批量，并一次将一个小批量送入IPCA算法。这 对于大型训练集和在线(即在新实例到来时动态运行)应用PCA很有用,请注意，你必须在每个小批量中调用partial_fit()方法，而不是在整个训练集中调用fit()方法。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hzh/miniforge3/envs/tfpy39/lib/python3.8/site-packages/sklearn/decomposition/_incremental_pca.py:348: RuntimeWarning: Mean of empty slice.\n",
      "  self.noise_variance_ = explained_variance[self.n_components_ :].mean()\n",
      "/Users/hzh/miniforge3/envs/tfpy39/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(500, 50)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 10\n",
    "inc_pca = IncrementalPCA(n_components=50)\n",
    "for X_batch in np.array_split(X, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_reduced = inc_pca.transform(X)\n",
    "X_reduced.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
